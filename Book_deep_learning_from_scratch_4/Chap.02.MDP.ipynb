{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25440360",
   "metadata": {},
   "source": [
    "----\n",
    "## Chap 02. Markov Descision Process (MDP)\n",
    "\n",
    "A probabilistic model of a sequential decision problem, where states can be perceived exactly, and the current state and action selected determine a probability distribution on future states. Essentially, the outcome of applying an action to a state depends only on the current action and state (and not on preceding actions or states).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567c970",
   "metadata": {},
   "source": [
    "### - **State** ‚Äî the state of the agent in the environment. $ $\n",
    "  - Deterministic :\n",
    "    - state transition function: $ s' = f(s, a)$\n",
    "  - Stochastic:\n",
    "    - state transition probability: $p(s' | s, a) $ << Markov property\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912435e",
   "metadata": {},
   "source": [
    "### -  <font color=\"blue\">**Reward (R)** </font> $ $\n",
    "  - For each action selected by the agent the environment provides a reward. Usually a scalar value.\n",
    "  - Immediate feedback that can be obtained from a given state\n",
    "  - Deterministic:\n",
    "    - Reward function: $r(s, a, s')$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fdf7d",
   "metadata": {},
   "source": [
    "### - <font color = \"blue\">**Policy ($\\pi$)**</font>\n",
    "  - The decision-making function (control strategy) of the agent, which represents a mapping from situations to actions.\n",
    "  - Rewards that can be expected from a given state: Expectation value\n",
    "  - Determisitic:\n",
    "    - $ Œ± = Œº (s) $ : output is an action.\n",
    "  - Probabilistic:\n",
    "    - $ \\pi (a| s)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b101a",
   "metadata": {},
   "source": [
    "### - <font color=\"red\">**Markov decision process (MDP)** </font>\n",
    "  - A probabilistic model of a sequential decision problem, where states can be perceived exactly, and the current state and action selected determine a probability distribution on future states. Essentially, the outcome of applying an action to a state depends only on the current action and state (and not on preceding actions or states).\n",
    "  - Episodic Task\n",
    "  - Continous Task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428432d0",
   "metadata": {},
   "source": [
    "### - <font color=\"blue\">**Gain (G)** </font>\n",
    "  - Total rewards that can be received from a given state to the end state in an episode\n",
    "  - $ G_t = R_t + ùõÑR_{t+1} + ùõÑ^{2}R_{t+2} + ùõÑ^{3}R_{t+3} + ...$\n",
    "  - $ G_t = R_t + ùõÑG_{t+1} $\n",
    "  - $Œ≥ $: Discount rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35a3a6",
   "metadata": {},
   "source": [
    "### - <font color = \"blue\">**State value Function**</font>\n",
    "  - It is the expected return (cumulative reward)starting from the state s following policy, œÄ.\n",
    "  - <font color=\"red\"> $$ V_{\\pi}(s) = E_{\\pi}[ \\sum_{t=0}^{T-1}Œ≥^{t}r_{t}|s_{t} = s ]$$</font>\n",
    "  - Œ≥ is the discount factor that determines how far future rewards are taken into account in the return.\n",
    "  - The total cumulative reward from timestep t can be written using goal G as shown below:\n",
    "  - <font color=\"red\">$$ V_{\\pi}(s) = E_{\\pi}[ G_t|s_{t} = s ] $$</font>\n",
    "  - <font color=\"red\">$$ V_{\\pi}(s) = E[G_t|S_{t} = s, \\pi] $$</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
