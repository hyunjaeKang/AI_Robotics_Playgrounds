{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab9d10c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cahp 03. Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09deff9",
   "metadata": {},
   "source": [
    "### Markov decision process (MDP):\n",
    "- <font color=\"red\">State: Deterministic</font>\n",
    "- <font color=\"red\">Agent: Deterministic</font>\n",
    "- $V_{\\pi}(s) = E_{\\pi}[\\sum_{t=0}^{T-1}Î³^{t}r_{t}|s_{t} = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbd212",
   "metadata": {},
   "source": [
    "### Bellman Equation: $ $\n",
    "- <font color=\"red\">State: Deterministic</font>\n",
    "- <font color=\"red\">Agent: Probabilistic</font>\n",
    "- Prestudy:\n",
    "    - Expectation value:\n",
    "        - $E[x] = \\sum_{x}xp(x)$\n",
    "        - $E[r(x, y)] = \\sum \\sum p(x, y)r(x, y) = \\sum \\sum p(x) p(y|x)r(x, y)$\n",
    "    - Reward for continous task\n",
    "        - $ G_t = R_t + ğ›„R_{t+1} + ğ›„^{2}R_{t+2} + ğ›„^{3}R_{t+3} + ...$\n",
    "        - $ G_t = R_t + ğ›„G_{t+1} $\n",
    "    - State value equation\n",
    "        - $ v_{\\pi}(s) = E_{\\pi}[G_t|S_{t} = s]$\n",
    "    - Action value equation\n",
    "        - $ q_\\pi(s, a) = E_{\\pi}[G_t|S_t=s, A_t = a]$\n",
    "- Bellman equation- State value equation\n",
    "    - $ v_{\\pi}(s) = E_{\\pi}[R_t + ğ›„G_{t+1}|S_{t} = s]$\n",
    "    - $ v_{\\pi}(s) = E_{\\pi}[R_t|S_{t} = s] + ğ›„E_{\\pi}[G_{t+1}|S_{t} = s]$\n",
    "    - $ v_{\\pi}(s) = \\sum_{a,s'} \\pi(a|s)p(s'|s, a)r(s, a, s') + ğ›„\\sum_{a, s'}\\pi(a|s)p(s'|s,a)v_{\\pi}(s')$\n",
    "    - <font color=\"red\">$$ v_{\\pi}(s) = \\sum_{a,s'} \\pi(a|s)p(s'|s, a) \\bigg(r(s, a, s') + ğ›„v_{\\pi}(s')\\\\bigg)$$ </font>\n",
    "\n",
    "- Bellman equation- Action value equation\n",
    "    - State value equation - Action value equation:\n",
    "        - $v_{\\pi}(s) = \\sum_{a}\\pi(a|s)q_{\\pi}(s,a)$\n",
    "    - $ q_\\pi(s, a) = E_{\\pi}[G_t|S_t=s, A_t = a]$\n",
    "    - $ q_\\pi(s, a) = E_{\\pi}[R_t + ğ›„G_{t+1}|S_t=s, A_t = a]$\n",
    "    - $ q_\\pi(s, a) = E_{\\pi}[R_t|S_t=s, A_t = a] + ğ›„E_{\\pi}[G_{t+1}|S_t=s, A_t = a]$\n",
    "    - $ q_\\pi(s, a) = \\sum_{s'}p(s'|s,a)r(s, a, s') + ğ›„\\sum_{s'}p(s'|s, a)E_{\\pi}[G_{t+1}|S_{t+1}=s]$\n",
    "    - $ q_\\pi(s, a) = \\sum_{s'}p(s'|s,a)\\bigg(r(s, a, s') + ğ›„E_{\\pi}[G_{t+1}|S_{t+1}=s]\\bigg)$\n",
    "    - $ q_\\pi(s, a) = \\sum_{s'}p(s'|s,a)\\bigg(r(s, a, s') + ğ›„v_\\pi(s')\\bigg)$\n",
    "    - <font color=\"red\">$$ q_\\pi(s, a) = \\sum_{s'}p(s'|s,a)\\bigg(r(s, a, s') + ğ›„\\sum_{a'}\\pi(a'|s')q_{\\pi}(s',a')\\bigg)$$</font>\n",
    "\n",
    "- Bellman optimality equation\n",
    "    - State value equation\n",
    "        - $v_{\\pi}(s) = \\sum_{a,s'} \\pi(a|s)p(s'|s, a) \\bigg(r(s, a, s') + ğ›„v_{\\pi}(s') \\bigg)$\n",
    "        - $ v_{\\pi}(s) = \\sum_{a} \\pi(a|s)\\sum_{s'}p(s'|s, a) \\bigg(r(s, a, s') + ğ›„v_{\\pi}(s') \\bigg)$\n",
    "        - <font color=\"red\">$$ v_{\\pi}(s) = \\underset{a}{\\operatorname{argmax}}\\sum\\limits_{s'}p(s'|s, a) \\bigg(r(s, a, s') + ğ›„v_{\\pi}(s') \\bigg)$$</font>\n",
    "    \n",
    "    - Action value equation\n",
    "        - $ q_\\pi(s, a) = \\sum_{s'}p(s'|s,a)\\bigg(r(s, a, s') + ğ›„\\sum_{a'}\\pi(a'|s')q_{\\pi}(s',a')\\bigg)$\n",
    "        - $ q_{\\pi}(s, a) = \\sum_{s'}p(s'|s,a)\\bigg(r(s, a, s') + ğ›„\\sum_{a'}\\pi_{\\pi}(a'|s')q_{\\pi}(s',a') \\bigg)$\n",
    "        - <font color=\"red\">$$ q_{\\pi}(s, a) = \\sum\\limits_{s'} p(s'|s,a)\\bigg(r(s, a, s') + ğ›„\\underset{a'}{\\operatorname{argmax}}q_{\\pi}(s',a')\\bigg)$$ </font>\n",
    "\n",
    "- Optimal policy\n",
    "    - $ \\mu_{\\pi}(s) = \\underset{a}{\\operatorname{argmax}} q_{\\pi}(s, a)$\n",
    "        - $ q_\\pi(s, a) = \\sum_{s'}p(s'|s,a)\\bigg(r(s, a, s') + ğ›„v_\\pi(s')\\bigg)$\n",
    "    - <font color=\"red\"> $$ \\mu_{\\pi}(s) = \\underset{a}{\\operatorname{argmax}} \\sum\\limits_{s'}p(s'|s,a)\\bigg(r(s, a, s') + ğ›„v_{pi}(s')\\bigg) $$</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
