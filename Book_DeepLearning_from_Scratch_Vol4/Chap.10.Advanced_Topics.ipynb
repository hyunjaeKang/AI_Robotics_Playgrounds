{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62666639",
   "metadata": {},
   "source": [
    "----\n",
    "## Chap 10. Advanced Topics\n",
    "\n",
    "<figure>\n",
    "<img src = \"https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg\">\n",
    "<figcaption align = \"center\">Fig. A non-exhaustive, but useful taxonomy of algorithms in modern RL. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba37f93",
   "metadata": {},
   "source": [
    "----\n",
    "#### 10.1. Model Based RL : $ $\n",
    "- State transition function:  $p(s'|s, a)$\n",
    "- Reward function :  $r(s, a, s')$\n",
    "- Given Model\n",
    "    - Agent: Planning without action\n",
    "    - Dyanmic programming\n",
    "    - Example:\n",
    "        - Go Game\n",
    "        - AlphaGo, AlphaZero\n",
    "- Learn the Model  \n",
    "    - Learn the Model from experience\n",
    "    - A Learned Model can be utilized for\n",
    "        - Planning\n",
    "        - Policy evaulation\n",
    "        - Policy control (optimization)\n",
    "    - Example:\n",
    "        - World Models\n",
    "        - MBVE (Model-Based Value Estimation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafaa072",
   "metadata": {},
   "source": [
    "#### 10.2 Model Free RL\n",
    "- Ploicy Optimization\n",
    "    - Policy Gradient\n",
    "    - A3C : Asynchronous Advantage Actor-Critic\n",
    "        - Efficient training with parallel mechanism\n",
    "        - More stable training due to parallel training mechanism\n",
    "            - This method is compatiable with on-policy\n",
    "            - Experience Replay from DQN to reduce correlation betweentraning data : not compatible with on-policy\n",
    "    - A2C : Advantage Actor-Critic\n",
    "        - Easy to implement compared to A3C\n",
    "    - DDPG : Deep Deterministic Policy Gradient Method\n",
    "        - Algorithm for continous action space\n",
    "        - $\\mu_{\\theta}(s) = a$ : deterministic\n",
    "            - update θ to maximize Q\n",
    "        - $Q_{\\phi}(s,a)$\n",
    "            - update $\\phi$ with DQN\n",
    "            - DQN : $Q_{\\phi} → R_t + \\gamma \\underset{a}{\\operatorname{max}}Q_{\\phi}(S_{t_1}, a)$\n",
    "            - DDPG: $\\underset{a}{\\operatorname{max}}Q_{\\phi}(S_{t_1}, a) \\simeq Q_{\\phi}(s, \\mu_{\\theta}(s))$\n",
    "    - TRPO : Trust Region Policy Optimization\n",
    "    - PPO : Proximal Policy Optimization\n",
    "\n",
    "- Value Grident(Optimization) : Q-Learning\n",
    "    - DQN\n",
    "    - Prioritized Experience Replay\n",
    "    - Duleing DQN\n",
    "    - Categorical DQN\n",
    "        - DQN : $Q_{\\pi}(s,a) = E_{\\pi}[G_t|S_t = s, A_t = a]$ : Estimated Value\n",
    "        - Categorical DQN : distribution reinforcement learning\n",
    "    - Noisy Network:\n",
    "        - DQN: action is determined by ϵ-Greedy approach\n",
    "            - ϵ : exploration : random selection\n",
    "            - (1-ϵ): exploitation\n",
    "        - withoud ϵ, using a greedy policy with noisy network\n",
    "    - Rainbow:\n",
    "        - Combination of (Double DQN, Prioritized Experience Replay, Duleing DQN, Categorical DQN, Noisy Network)\n",
    "    - Ape-X : Multiple Rainbos on multiple CPU\n",
    "    - R2D2 : Ape-X + RNN\n",
    "        - Recurrent, Replay\n",
    "        - Distribution, Deep Q-Network (DQN)\n",
    "    - NGU (Never Give Up)\n",
    "        - Apply intrinsic reward\n",
    "    - Agent57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b704f1",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Thoughts\n",
    "\n",
    "- Q Learning : Off-policy\n",
    "    - Utilize the past data (data trajectory)\n",
    "\n",
    "- Offline reinforcement learning:\n",
    "    - Training model without any interaction with environment.\n",
    "    - Only utilized data set.\n",
    "\n",
    "- Monte Carlo, TD : Model Free"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
